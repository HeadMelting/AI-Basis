{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,rand\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet,self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        # self.relu1 = nn.ReLU()\n",
    "        # self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        # self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        # output = nn.Linear(in_features=14,output)\n",
    "\n",
    "        # output size = (N - F + padding) / stride + 1\n",
    "         \n",
    "        self.clf = nn.Sequential(\n",
    "         # output size = (28 + 2(pad) - 3 ) / 1 + 1 = 28\n",
    "         nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1),\n",
    "         nn.ReLU(inplace=True),\n",
    "         # output size = (28 - 3) / 1 + 1 = 26\n",
    "         nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1),\n",
    "         nn.ReLU(inplace=True),\n",
    "         # output size = 26 / 2 = 13\n",
    "         nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=64*13*13, out_features=100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=100,out_features=10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self,input):\n",
    "        output = self.clf(input)\n",
    "        output = output.view(output.size()[0],-1)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type: <class 'torch.Tensor'> output: tensor([[0.0901, 0.1023, 0.1001, 0.1067, 0.0984, 0.1043, 0.1117, 0.0981, 0.0896,\n",
      "         0.0987]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = rand(1,1,28,28)\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print('output type:', type(output), 'output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_preprocessed_data(images, labels):\n",
    "\n",
    "    images = np.array(images/255.0, dtype=np.float32)\n",
    "    lables = np.array(labels, dtype=np.float32)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# 0~1사이값 float32로 변경하는 함수 호출 한 뒤 OHE 적용\n",
    "def get_preprocessed_ohe(images, labels):\n",
    "    images, labels = get_preprocessed_data(images,labels)\n",
    "    oh_labels = nn.functional.one_hot(labels,len(torch.unique(labels)))\n",
    "    return images, oh_labels\n",
    "\n",
    "# 학습 검증 테스트 데이터 세트에 전처리 및 ohe 적용한 뒤 반환\n",
    "def get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n",
    "    train_images, train_oh_lables = get_preprocessed_ohe(train_images,train_labels)\n",
    "    test_images, test_oh_labels = get_preprocessed_ohe(test_images,test_labels)\n",
    "\n",
    "    tr_images, val_images, train_oh_lables, val_oh_labels = train_test_split(train_images, train_oh_lables,test_size=valid_size,random_state=random_state)\n",
    "\n",
    "    return (tr_images, train_oh_lables), (val_images, val_oh_labels), (test_images, test_oh_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([60000]) torch.Size([10000, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "transform = ToTensor()\n",
    "\n",
    "train_data = MNIST(root='data',download=True,transform=transform,train=True)\n",
    "test_data = MNIST(root='data',download=True,transform=transform,train=False)\n",
    "\n",
    "train_images, train_labels = train_data.data.float()/255.0 , train_data.targets\n",
    "test_images, test_labels = test_data.data.float()/255.0 , test_data.targets\n",
    "\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "\n",
    "(tr_images, train_oh_lables), (val_images, val_oh_labels), (test_images, test_oh_labels) = get_train_valid_test_set(train_images,train_labels,test_images,test_labels,valid_size=0.15,random_state=2021)\n",
    "\n",
    "train_data_loader =  torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=False)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 469\n",
      "1 / 469\n",
      "2 / 469\n",
      "3 / 469\n",
      "4 / 469\n",
      "5 / 469\n",
      "6 / 469\n",
      "7 / 469\n",
      "8 / 469\n",
      "9 / 469\n",
      "10 / 469\n",
      "11 / 469\n",
      "12 / 469\n",
      "13 / 469\n",
      "14 / 469\n",
      "15 / 469\n",
      "16 / 469\n",
      "17 / 469\n",
      "18 / 469\n",
      "19 / 469\n",
      "20 / 469\n",
      "21 / 469\n",
      "22 / 469\n",
      "23 / 469\n",
      "24 / 469\n",
      "25 / 469\n",
      "26 / 469\n",
      "27 / 469\n",
      "28 / 469\n",
      "29 / 469\n",
      "30 / 469\n",
      "31 / 469\n",
      "32 / 469\n",
      "33 / 469\n",
      "34 / 469\n",
      "35 / 469\n",
      "36 / 469\n",
      "37 / 469\n",
      "38 / 469\n",
      "39 / 469\n",
      "40 / 469\n",
      "41 / 469\n",
      "42 / 469\n",
      "43 / 469\n",
      "44 / 469\n",
      "45 / 469\n",
      "46 / 469\n",
      "47 / 469\n",
      "48 / 469\n",
      "49 / 469\n",
      "50 / 469\n",
      "51 / 469\n",
      "52 / 469\n",
      "53 / 469\n",
      "54 / 469\n",
      "55 / 469\n",
      "56 / 469\n",
      "57 / 469\n",
      "58 / 469\n",
      "59 / 469\n",
      "60 / 469\n",
      "61 / 469\n",
      "62 / 469\n",
      "63 / 469\n",
      "64 / 469\n",
      "65 / 469\n",
      "66 / 469\n",
      "67 / 469\n",
      "68 / 469\n",
      "69 / 469\n",
      "70 / 469\n",
      "71 / 469\n",
      "72 / 469\n",
      "73 / 469\n",
      "74 / 469\n",
      "75 / 469\n",
      "76 / 469\n",
      "77 / 469\n",
      "78 / 469\n",
      "79 / 469\n",
      "80 / 469\n",
      "81 / 469\n",
      "82 / 469\n",
      "83 / 469\n",
      "84 / 469\n",
      "85 / 469\n",
      "86 / 469\n",
      "87 / 469\n",
      "88 / 469\n",
      "89 / 469\n",
      "90 / 469\n",
      "91 / 469\n",
      "92 / 469\n",
      "93 / 469\n",
      "94 / 469\n",
      "95 / 469\n",
      "96 / 469\n",
      "97 / 469\n",
      "98 / 469\n",
      "99 / 469\n",
      "100 / 469\n",
      "101 / 469\n",
      "102 / 469\n",
      "103 / 469\n",
      "104 / 469\n",
      "105 / 469\n",
      "106 / 469\n",
      "107 / 469\n",
      "108 / 469\n",
      "109 / 469\n",
      "110 / 469\n",
      "111 / 469\n",
      "112 / 469\n",
      "113 / 469\n",
      "114 / 469\n",
      "115 / 469\n",
      "116 / 469\n",
      "117 / 469\n",
      "118 / 469\n",
      "119 / 469\n",
      "120 / 469\n",
      "121 / 469\n",
      "122 / 469\n",
      "123 / 469\n",
      "124 / 469\n",
      "125 / 469\n",
      "126 / 469\n",
      "127 / 469\n",
      "128 / 469\n",
      "129 / 469\n",
      "130 / 469\n",
      "131 / 469\n",
      "132 / 469\n",
      "133 / 469\n",
      "134 / 469\n",
      "135 / 469\n",
      "136 / 469\n",
      "137 / 469\n",
      "138 / 469\n",
      "139 / 469\n",
      "140 / 469\n",
      "141 / 469\n",
      "142 / 469\n",
      "143 / 469\n",
      "144 / 469\n",
      "145 / 469\n",
      "146 / 469\n",
      "147 / 469\n",
      "148 / 469\n",
      "149 / 469\n",
      "150 / 469\n",
      "151 / 469\n",
      "152 / 469\n",
      "153 / 469\n",
      "154 / 469\n",
      "155 / 469\n",
      "156 / 469\n",
      "157 / 469\n",
      "158 / 469\n",
      "159 / 469\n",
      "160 / 469\n",
      "161 / 469\n",
      "162 / 469\n",
      "163 / 469\n",
      "164 / 469\n",
      "165 / 469\n",
      "166 / 469\n",
      "167 / 469\n",
      "168 / 469\n",
      "169 / 469\n",
      "170 / 469\n",
      "171 / 469\n",
      "172 / 469\n",
      "173 / 469\n",
      "174 / 469\n",
      "175 / 469\n",
      "176 / 469\n",
      "177 / 469\n",
      "178 / 469\n",
      "179 / 469\n",
      "180 / 469\n",
      "181 / 469\n",
      "182 / 469\n",
      "183 / 469\n",
      "184 / 469\n",
      "185 / 469\n",
      "186 / 469\n",
      "187 / 469\n",
      "188 / 469\n",
      "189 / 469\n",
      "190 / 469\n",
      "191 / 469\n",
      "192 / 469\n",
      "193 / 469\n",
      "194 / 469\n",
      "195 / 469\n",
      "196 / 469\n",
      "197 / 469\n",
      "198 / 469\n",
      "199 / 469\n",
      "200 / 469\n",
      "201 / 469\n",
      "202 / 469\n",
      "203 / 469\n",
      "204 / 469\n",
      "205 / 469\n",
      "206 / 469\n",
      "207 / 469\n",
      "208 / 469\n",
      "209 / 469\n",
      "210 / 469\n",
      "211 / 469\n",
      "212 / 469\n",
      "213 / 469\n",
      "214 / 469\n",
      "215 / 469\n",
      "216 / 469\n",
      "217 / 469\n",
      "218 / 469\n",
      "219 / 469\n",
      "220 / 469\n",
      "221 / 469\n",
      "222 / 469\n",
      "223 / 469\n",
      "224 / 469\n",
      "225 / 469\n",
      "226 / 469\n",
      "227 / 469\n",
      "228 / 469\n",
      "229 / 469\n",
      "230 / 469\n",
      "231 / 469\n",
      "232 / 469\n",
      "233 / 469\n",
      "234 / 469\n",
      "235 / 469\n",
      "236 / 469\n",
      "237 / 469\n",
      "238 / 469\n",
      "239 / 469\n",
      "240 / 469\n",
      "241 / 469\n",
      "242 / 469\n",
      "243 / 469\n",
      "244 / 469\n",
      "245 / 469\n",
      "246 / 469\n",
      "247 / 469\n",
      "248 / 469\n",
      "249 / 469\n",
      "250 / 469\n",
      "251 / 469\n",
      "252 / 469\n",
      "253 / 469\n",
      "254 / 469\n",
      "255 / 469\n",
      "256 / 469\n",
      "257 / 469\n",
      "258 / 469\n",
      "259 / 469\n",
      "260 / 469\n",
      "261 / 469\n",
      "262 / 469\n",
      "263 / 469\n",
      "264 / 469\n",
      "265 / 469\n",
      "266 / 469\n",
      "267 / 469\n",
      "268 / 469\n",
      "269 / 469\n",
      "270 / 469\n",
      "271 / 469\n",
      "272 / 469\n",
      "273 / 469\n",
      "274 / 469\n",
      "275 / 469\n",
      "276 / 469\n",
      "277 / 469\n",
      "278 / 469\n",
      "279 / 469\n",
      "280 / 469\n",
      "281 / 469\n",
      "282 / 469\n",
      "283 / 469\n",
      "284 / 469\n",
      "285 / 469\n",
      "286 / 469\n",
      "287 / 469\n",
      "288 / 469\n",
      "289 / 469\n",
      "290 / 469\n",
      "291 / 469\n",
      "292 / 469\n",
      "293 / 469\n",
      "294 / 469\n",
      "295 / 469\n",
      "296 / 469\n",
      "297 / 469\n",
      "298 / 469\n",
      "299 / 469\n",
      "300 / 469\n",
      "301 / 469\n",
      "302 / 469\n",
      "303 / 469\n",
      "304 / 469\n",
      "305 / 469\n",
      "306 / 469\n",
      "307 / 469\n",
      "308 / 469\n",
      "309 / 469\n",
      "310 / 469\n",
      "311 / 469\n",
      "312 / 469\n",
      "313 / 469\n",
      "314 / 469\n",
      "315 / 469\n",
      "316 / 469\n",
      "317 / 469\n",
      "318 / 469\n",
      "319 / 469\n",
      "320 / 469\n",
      "321 / 469\n",
      "322 / 469\n",
      "323 / 469\n",
      "324 / 469\n",
      "325 / 469\n",
      "326 / 469\n",
      "327 / 469\n",
      "328 / 469\n",
      "329 / 469\n",
      "330 / 469\n",
      "331 / 469\n",
      "332 / 469\n",
      "333 / 469\n",
      "334 / 469\n",
      "335 / 469\n",
      "336 / 469\n",
      "337 / 469\n",
      "338 / 469\n",
      "339 / 469\n",
      "340 / 469\n",
      "341 / 469\n",
      "342 / 469\n",
      "343 / 469\n",
      "344 / 469\n",
      "345 / 469\n",
      "346 / 469\n",
      "347 / 469\n",
      "348 / 469\n",
      "349 / 469\n",
      "350 / 469\n",
      "351 / 469\n",
      "352 / 469\n",
      "353 / 469\n",
      "354 / 469\n",
      "355 / 469\n",
      "356 / 469\n",
      "357 / 469\n",
      "358 / 469\n",
      "359 / 469\n",
      "360 / 469\n",
      "361 / 469\n",
      "362 / 469\n",
      "363 / 469\n",
      "364 / 469\n",
      "365 / 469\n",
      "366 / 469\n",
      "367 / 469\n",
      "368 / 469\n",
      "369 / 469\n",
      "370 / 469\n",
      "371 / 469\n",
      "372 / 469\n",
      "373 / 469\n",
      "374 / 469\n",
      "375 / 469\n",
      "376 / 469\n",
      "377 / 469\n",
      "378 / 469\n",
      "379 / 469\n",
      "380 / 469\n",
      "381 / 469\n",
      "382 / 469\n",
      "383 / 469\n",
      "384 / 469\n",
      "385 / 469\n",
      "386 / 469\n",
      "387 / 469\n",
      "388 / 469\n",
      "389 / 469\n",
      "390 / 469\n",
      "391 / 469\n",
      "392 / 469\n",
      "393 / 469\n",
      "394 / 469\n",
      "395 / 469\n",
      "396 / 469\n",
      "397 / 469\n",
      "398 / 469\n",
      "399 / 469\n",
      "400 / 469\n",
      "401 / 469\n",
      "402 / 469\n",
      "403 / 469\n",
      "404 / 469\n",
      "405 / 469\n",
      "406 / 469\n",
      "407 / 469\n",
      "408 / 469\n",
      "409 / 469\n",
      "410 / 469\n",
      "411 / 469\n",
      "412 / 469\n",
      "413 / 469\n",
      "414 / 469\n",
      "415 / 469\n",
      "416 / 469\n",
      "417 / 469\n",
      "418 / 469\n",
      "419 / 469\n",
      "420 / 469\n",
      "421 / 469\n",
      "422 / 469\n",
      "423 / 469\n",
      "424 / 469\n",
      "425 / 469\n",
      "426 / 469\n",
      "427 / 469\n",
      "428 / 469\n",
      "429 / 469\n",
      "430 / 469\n",
      "431 / 469\n",
      "432 / 469\n",
      "433 / 469\n",
      "434 / 469\n",
      "435 / 469\n",
      "436 / 469\n",
      "437 / 469\n",
      "438 / 469\n",
      "439 / 469\n",
      "440 / 469\n",
      "441 / 469\n",
      "442 / 469\n",
      "443 / 469\n",
      "444 / 469\n",
      "445 / 469\n",
      "446 / 469\n",
      "447 / 469\n",
      "448 / 469\n",
      "449 / 469\n",
      "450 / 469\n",
      "451 / 469\n",
      "452 / 469\n",
      "453 / 469\n",
      "454 / 469\n",
      "455 / 469\n",
      "456 / 469\n",
      "457 / 469\n",
      "458 / 469\n",
      "459 / 469\n",
      "460 / 469\n",
      "461 / 469\n",
      "462 / 469\n",
      "463 / 469\n",
      "464 / 469\n",
      "465 / 469\n",
      "466 / 469\n",
      "467 / 469\n",
      "468 / 469\n",
      "Test set: Epoch: 1, Average loss: 0.0001, Accuracy: 0.0125\n",
      "Test set: Epoch: 1, Average loss: 0.0003, Accuracy: 0.0248\n",
      "Test set: Epoch: 1, Average loss: 0.0004, Accuracy: 0.0373\n",
      "Test set: Epoch: 1, Average loss: 0.0006, Accuracy: 0.0497\n",
      "Test set: Epoch: 1, Average loss: 0.0007, Accuracy: 0.0618\n",
      "Test set: Epoch: 1, Average loss: 0.0009, Accuracy: 0.0744\n",
      "Test set: Epoch: 1, Average loss: 0.0010, Accuracy: 0.0864\n",
      "Test set: Epoch: 1, Average loss: 0.0012, Accuracy: 0.0991\n",
      "Test set: Epoch: 1, Average loss: 0.0013, Accuracy: 0.1116\n",
      "Test set: Epoch: 1, Average loss: 0.0015, Accuracy: 0.1243\n",
      "Test set: Epoch: 1, Average loss: 0.0016, Accuracy: 0.1367\n",
      "Test set: Epoch: 1, Average loss: 0.0018, Accuracy: 0.1489\n",
      "Test set: Epoch: 1, Average loss: 0.0019, Accuracy: 0.1613\n",
      "Test set: Epoch: 1, Average loss: 0.0021, Accuracy: 0.1736\n",
      "Test set: Epoch: 1, Average loss: 0.0022, Accuracy: 0.1862\n",
      "Test set: Epoch: 1, Average loss: 0.0024, Accuracy: 0.1984\n",
      "Test set: Epoch: 1, Average loss: 0.0025, Accuracy: 0.2107\n",
      "Test set: Epoch: 1, Average loss: 0.0027, Accuracy: 0.2233\n",
      "Test set: Epoch: 1, Average loss: 0.0028, Accuracy: 0.2358\n",
      "Test set: Epoch: 1, Average loss: 0.0030, Accuracy: 0.2484\n",
      "Test set: Epoch: 1, Average loss: 0.0031, Accuracy: 0.2611\n",
      "Test set: Epoch: 1, Average loss: 0.0033, Accuracy: 0.2737\n",
      "Test set: Epoch: 1, Average loss: 0.0034, Accuracy: 0.2860\n",
      "Test set: Epoch: 1, Average loss: 0.0036, Accuracy: 0.2986\n",
      "Test set: Epoch: 1, Average loss: 0.0037, Accuracy: 0.3111\n",
      "Test set: Epoch: 1, Average loss: 0.0039, Accuracy: 0.3237\n",
      "Test set: Epoch: 1, Average loss: 0.0040, Accuracy: 0.3360\n",
      "Test set: Epoch: 1, Average loss: 0.0042, Accuracy: 0.3483\n",
      "Test set: Epoch: 1, Average loss: 0.0043, Accuracy: 0.3606\n",
      "Test set: Epoch: 1, Average loss: 0.0045, Accuracy: 0.3730\n",
      "Test set: Epoch: 1, Average loss: 0.0046, Accuracy: 0.3857\n",
      "Test set: Epoch: 1, Average loss: 0.0048, Accuracy: 0.3982\n",
      "Test set: Epoch: 1, Average loss: 0.0049, Accuracy: 0.4106\n",
      "Test set: Epoch: 1, Average loss: 0.0051, Accuracy: 0.4229\n",
      "Test set: Epoch: 1, Average loss: 0.0052, Accuracy: 0.4354\n",
      "Test set: Epoch: 1, Average loss: 0.0054, Accuracy: 0.4474\n",
      "Test set: Epoch: 1, Average loss: 0.0055, Accuracy: 0.4598\n",
      "Test set: Epoch: 1, Average loss: 0.0057, Accuracy: 0.4722\n",
      "Test set: Epoch: 1, Average loss: 0.0058, Accuracy: 0.4846\n",
      "Test set: Epoch: 1, Average loss: 0.0060, Accuracy: 0.4964\n",
      "Test set: Epoch: 1, Average loss: 0.0061, Accuracy: 0.5090\n",
      "Test set: Epoch: 1, Average loss: 0.0063, Accuracy: 0.5216\n",
      "Test set: Epoch: 1, Average loss: 0.0064, Accuracy: 0.5341\n",
      "Test set: Epoch: 1, Average loss: 0.0066, Accuracy: 0.5465\n",
      "Test set: Epoch: 1, Average loss: 0.0067, Accuracy: 0.5591\n",
      "Test set: Epoch: 1, Average loss: 0.0069, Accuracy: 0.5716\n",
      "Test set: Epoch: 1, Average loss: 0.0070, Accuracy: 0.5844\n",
      "Test set: Epoch: 1, Average loss: 0.0072, Accuracy: 0.5967\n",
      "Test set: Epoch: 1, Average loss: 0.0073, Accuracy: 0.6090\n",
      "Test set: Epoch: 1, Average loss: 0.0075, Accuracy: 0.6212\n",
      "Test set: Epoch: 1, Average loss: 0.0076, Accuracy: 0.6339\n",
      "Test set: Epoch: 1, Average loss: 0.0078, Accuracy: 0.6463\n",
      "Test set: Epoch: 1, Average loss: 0.0079, Accuracy: 0.6589\n",
      "Test set: Epoch: 1, Average loss: 0.0081, Accuracy: 0.6715\n",
      "Test set: Epoch: 1, Average loss: 0.0082, Accuracy: 0.6840\n",
      "Test set: Epoch: 1, Average loss: 0.0084, Accuracy: 0.6966\n",
      "Test set: Epoch: 1, Average loss: 0.0085, Accuracy: 0.7092\n",
      "Test set: Epoch: 1, Average loss: 0.0087, Accuracy: 0.7215\n",
      "Test set: Epoch: 1, Average loss: 0.0088, Accuracy: 0.7342\n",
      "Test set: Epoch: 1, Average loss: 0.0089, Accuracy: 0.7466\n",
      "Test set: Epoch: 1, Average loss: 0.0091, Accuracy: 0.7590\n",
      "Test set: Epoch: 1, Average loss: 0.0092, Accuracy: 0.7716\n",
      "Test set: Epoch: 1, Average loss: 0.0094, Accuracy: 0.7837\n",
      "Test set: Epoch: 1, Average loss: 0.0095, Accuracy: 0.7960\n",
      "Test set: Epoch: 1, Average loss: 0.0097, Accuracy: 0.8084\n",
      "Test set: Epoch: 1, Average loss: 0.0098, Accuracy: 0.8207\n",
      "Test set: Epoch: 1, Average loss: 0.0100, Accuracy: 0.8330\n",
      "Test set: Epoch: 1, Average loss: 0.0101, Accuracy: 0.8457\n",
      "Test set: Epoch: 1, Average loss: 0.0103, Accuracy: 0.8582\n",
      "Test set: Epoch: 1, Average loss: 0.0104, Accuracy: 0.8707\n",
      "Test set: Epoch: 1, Average loss: 0.0106, Accuracy: 0.8833\n",
      "Test set: Epoch: 1, Average loss: 0.0107, Accuracy: 0.8960\n",
      "Test set: Epoch: 1, Average loss: 0.0109, Accuracy: 0.9085\n",
      "Test set: Epoch: 1, Average loss: 0.0110, Accuracy: 0.9207\n",
      "Test set: Epoch: 1, Average loss: 0.0112, Accuracy: 0.9331\n",
      "Test set: Epoch: 1, Average loss: 0.0113, Accuracy: 0.9456\n",
      "Test set: Epoch: 1, Average loss: 0.0115, Accuracy: 0.9580\n",
      "Test set: Epoch: 1, Average loss: 0.0116, Accuracy: 0.9702\n",
      "Test set: Epoch: 1, Average loss: 0.0118, Accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):    \n",
    "    model.train()\n",
    "    for num_iter, (images, labels) in enumerate(train_data_loader):\n",
    "        print(num_iter,\"/\",len(train_data_loader))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    for (images, labels) in test_data_loader:\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "\n",
    "        print('Test set: Epoch: {}, Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "        epoch,\n",
    "        test_loss / len(test_data_loader.dataset),\n",
    "        correct.float() / len(test_data_loader.dataset)\n",
    "    ))\n",
    "\n",
    "\n",
    "for epoch in range(1,2):\n",
    "    train(epoch)\n",
    "    acc = eval(epoch)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = model(test_images)\n",
    "    loss = loss_function(outputs,test_labels)\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(test_labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5b916051ec391ef3c1c0123575e59cad2c35863d294dd079abc5845c0e5babb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
